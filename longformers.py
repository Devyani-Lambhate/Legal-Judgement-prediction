# -*- coding: utf-8 -*-
"""Copy of Longformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15pVYYCZY3W9OWL9F7_H9d10FQswAUuVE
"""

#!pip install datasets
 #!pip install wandb
 #!pip install transformers

import pandas as pd
from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig
import torch.nn as nn
import torch
#from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm
import wandb
import os
from datasets import Dataset


import json
# Libraries
import matplotlib.pyplot as plt# Preliminaries
from torchtext.data import Field, TabularDataset, BucketIterator, Iterator
import numpy as np

path='ECHR_Dataset/EN_train/'
path_test='ECHR_Dataset/EN_test/'

def listToString(s):  
    
    # initialize an empty string 
    str1 = ""  
    
    # traverse in the string   
    for ele in s:  
        str1 += ele   
    
    # return string   
    return str1  
    

def prepare_data():

		DATASET=[]
		for fname in sorted(os.listdir(path)):
			with open(path+fname, "r") as read_file:
				data = json.load(read_file)
				DATASET.append(data)
		print(DATASET[0])
		
		TEXT=[]
		VIOLATED_ARTICLES=[]
		
		for i in range(len(DATASET)):
			a=DATASET[i]['TEXT']
			TEXT.append(listToString(a))
			VIOLATED_ARTICLES.append(DATASET[i]['VIOLATED_ARTICLES'])
		

		#TEXT= DATASET[]['TEXT']

		#print(train_x['TEXT'])
		#print(train_x['VIOLATED_ARTICLES'])

		def check(string, sub_str): 
		    if (string.find(sub_str) == -1): 
		        return 0
		    else: 
		        return 1 

		BINARY_CONCLUSION=[]
		for item in VIOLATED_ARTICLES:
			#print(item)
			if(item==[]):
				BINARY_CONCLUSION.append(0)
			else:
				BINARY_CONCLUSION.append(1)






			
		#train_x.insert(16, "BINARY_CONCLUSION", BINARY_CONCLUSION, True)

		
		

		return TEXT,BINARY_CONCLUSION


contents,labels=prepare_data()

config = LongformerConfig()

config

#from google.colab import drive
#drive.mount('/content/drive')

#train_data, test_data = datasets.load_dataset('imdb', split =['train', 'test'], 
#                                             cache_dir='/media/data_files/github/website_tutorials/data')
from sklearn.model_selection import train_test_split
data_path='train/'
#neg_files = data_path + 'neg'
#pos_files = data_path + 'pos'
'''
def get_contents(path):
  contents = []
  c=0
  for file_name in os.listdir(path):
    with open(path + '/' + file_name,'r') as f:
      pos_content = ''
      for line in f:
        pos_content = pos_content + line.strip() + ' '
      c+=1
      contents.append(pos_content.strip())
      if c%100==0:
        print(c)
  return contents
'''
#pos_contents = get_contents(pos_files)
#neg_contents = get_contents(neg_files)
#contents=train_x['TEXT']
#labels=train_x['BINARY_CONCLUSION']
print(labels)

#labels = torch.cat([torch.ones(len(pos_contents),dtype = torch.long),torch.zeros(len(neg_contents),dtype = torch.long)])
labels = torch.tensor(labels,dtype = torch.long)
print(contents[0],labels)


class LegalDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
 
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item
 
    def __len__(self):
        return self.labels.shape[0]

def tokenization(batched_text):
    return tokenizer(batched_text, padding = 'max_length', truncation=True, max_length = 4096)
 
tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')
train_x,test_x,train_y,test_y = train_test_split(contents,labels,test_size=0.15,stratify=labels)
 
train_encodings = tokenization(train_x)
test_encodings = tokenization(test_x)
 
train_data = LegalDataset(train_encodings, train_y)
test_data = LegalDataset(test_encodings, test_y)

print(train_data)







# load model and tokenizer and define length of the text sequence
model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',
                                                           gradient_checkpointing=False,
                                                           attention_window = 512)

model.config

# define a function that will tokenize the model, and will return the relevant inputs for the model
def tokenization(batched_text):
    return tokenizer(batched_text['text'], padding = 'max_length', truncation=True)

# define the training arguments
training_args = TrainingArguments(
    output_dir = 'final_model/output',
    num_train_epochs = 3,
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 8,    
    per_device_eval_batch_size= 4,
    evaluation_strategy = "epoch",
    disable_tqdm = False, 
    load_best_model_at_end=True,
    warmup_steps=200,
    weight_decay=0.01,
    logging_steps = 4,
    fp16 = False ,
    logging_dir='final_model/logs',
    dataloader_num_workers = 0,
    run_name = 'longformer-classification-updated-rtx3090_paper_replication_2_warm'
)

# define accuracy metrics
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }
    
# instantiate the trainer class and check for available devices
trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_data,
    eval_dataset=test_data
)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

# save the best model

trainer.train()
trainer.save_model('final_model/saved_model')
trainer.evaluate()
